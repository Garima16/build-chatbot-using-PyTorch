{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aHzRzqYNlmGL"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import codecs\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Xt5IarmDl_LE",
    "outputId": "0c972d35-d3bb-46db-ade0-4099304d25b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# to provide easy switching between cpu and gpu\n",
    "CUDA = print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DxExTYw-3mWY"
   },
   "source": [
    "# Part 1 :  **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "vh_ALj0QkiTM",
    "outputId": "b6fc8a04-d4ea-445c-cde0-655fc5a9bfe0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\n",
      "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L198', 'L199']\n",
      "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L200', 'L201', 'L202', 'L203']\n",
      "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L204', 'L205', 'L206']\n",
      "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L207', 'L208']\n",
      "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L271', 'L272', 'L273', 'L274', 'L275']\n",
      "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L276', 'L277']\n",
      "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L280', 'L281']\n",
      "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L363', 'L364']\n",
      "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L365', 'L366']\n"
     ]
    }
   ],
   "source": [
    "with open('movie_conversations.txt','r') as conv_file:\n",
    "  convs = conv_file.readlines()\n",
    "for conv in convs[:10]:\n",
    "  print(conv.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "colab_type": "code",
    "id": "7dNqOrE3nXNS",
    "outputId": "c8833928-4e40-4f5c-8d60-9c05dbe807a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!'\n",
      "b'L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!'\n",
      "b'L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.'\n",
      "b'L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?'\n",
      "b\"L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\"\n",
      "b'L924 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ Wow'\n",
      "b\"L872 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Okay -- you're gonna need to learn how to lie.\"\n",
      "b'L871 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ No'\n",
      "b'L870 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I\\'m kidding.  You know how sometimes you just become this \"persona\"?  And you don\\'t know how to quit?'\n",
      "b'L869 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Like my fear of wearing pastels?'\n"
     ]
    }
   ],
   "source": [
    "with open('movie_lines.txt', 'r', errors='replace') as lines_file:\n",
    "  lines = lines_file.readlines()\n",
    "for line in lines[:10]:\n",
    "  print(line.encode('utf-8').strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bZEl5-KmmC3D"
   },
   "outputs": [],
   "source": [
    "line_fields = ['lineID', 'charID', 'movieID', 'char_name', 'text']\n",
    "lines = {}\n",
    "with open('movie_lines.txt','r',encoding='iso-8859-1') as file: #encodes ASCII chars, single Byte encoding method\n",
    "#   head = [next(file) for x in range(10)]\n",
    "  for line in file:\n",
    "    vals = line.strip().split('+++$+++')\n",
    "    lineID = vals[0].strip(' ')\n",
    "    line_obj = {}\n",
    "    line_obj['lineID'] = lineID\n",
    "    for i in range(1,len(vals)):\n",
    "      line_obj[line_fields[i]] = vals[i]\n",
    "    lines[line_obj['lineID']] = line_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HVLXHXmhGWPh"
   },
   "outputs": [],
   "source": [
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "DEOdehHqvZic",
    "outputId": "511298e6-753c-4530-f477-e5ad5a83a39e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'char1ID': 'u0 ',\n",
      "  'char2ID': ' u2 ',\n",
      "  'dialoguesIDs': \" ['L194', 'L195', 'L196', 'L197']\",\n",
      "  'lines': [{'charID': ' u0 ',\n",
      "             'char_name': ' BIANCA ',\n",
      "             'lineID': 'L194',\n",
      "             'movieID': ' m0 ',\n",
      "             'text': ' Can we make this quick?  Roxanne Korrine and Andrew '\n",
      "                     'Barrett are having an incredibly horrendous public '\n",
      "                     'break- up on the quad.  Again.'},\n",
      "            {'charID': ' u2 ',\n",
      "             'char_name': ' CAMERON ',\n",
      "             'lineID': 'L195',\n",
      "             'movieID': ' m0 ',\n",
      "             'text': \" Well, I thought we'd start with pronunciation, if \"\n",
      "                     \"that's okay with you.\"},\n",
      "            {'charID': ' u0 ',\n",
      "             'char_name': ' BIANCA ',\n",
      "             'lineID': 'L196',\n",
      "             'movieID': ' m0 ',\n",
      "             'text': ' Not the hacking and gagging and spitting part.  '\n",
      "                     'Please.'},\n",
      "            {'charID': ' u2 ',\n",
      "             'char_name': ' CAMERON ',\n",
      "             'lineID': 'L197',\n",
      "             'movieID': ' m0 ',\n",
      "             'text': \" Okay... then how 'bout we try out some French cuisine.  \"\n",
      "                     'Saturday?  Night?'}],\n",
      "  'movieID': ' m0 '},\n",
      " {'char1ID': 'u0 ',\n",
      "  'char2ID': ' u2 ',\n",
      "  'dialoguesIDs': \" ['L198', 'L199']\",\n",
      "  'lines': [{'charID': ' u0 ',\n",
      "             'char_name': ' BIANCA ',\n",
      "             'lineID': 'L198',\n",
      "             'movieID': ' m0 ',\n",
      "             'text': \" You're asking me out.  That's so cute. What's your name \"\n",
      "                     'again?'},\n",
      "            {'charID': ' u2 ',\n",
      "             'char_name': ' CAMERON ',\n",
      "             'lineID': 'L199',\n",
      "             'movieID': ' m0 ',\n",
      "             'text': ' Forget it.'}],\n",
      "  'movieID': ' m0 '},\n",
      " {'char1ID': 'u0 ',\n",
      "  'char2ID': ' u2 ',\n",
      "  'dialoguesIDs': \" ['L200', 'L201', 'L202', 'L203']\",\n",
      "  'lines': [{'charID': ' u0 ',\n",
      "             'char_name': ' BIANCA ',\n",
      "             'lineID': 'L200',\n",
      "             'movieID': ' m0 ',\n",
      "             'text': \" No, no, it's my fault -- we didn't have a proper \"\n",
      "                     'introduction ---'},\n",
      "            {'charID': ' u2 ',\n",
      "             'char_name': ' CAMERON ',\n",
      "             'lineID': 'L201',\n",
      "             'movieID': ' m0 ',\n",
      "             'text': ' Cameron.'},\n",
      "            {'charID': ' u0 ',\n",
      "             'char_name': ' BIANCA ',\n",
      "             'lineID': 'L202',\n",
      "             'movieID': ' m0 ',\n",
      "             'text': \" The thing is, Cameron -- I'm at the mercy of a \"\n",
      "                     'particularly hideous breed of loser.  My sister.  I '\n",
      "                     \"can't date until she does.\"},\n",
      "            {'charID': ' u2 ',\n",
      "             'char_name': ' CAMERON ',\n",
      "             'lineID': 'L203',\n",
      "             'movieID': ' m0 ',\n",
      "             'text': ' Seems like she could get a date easy enough...'}],\n",
      "  'movieID': ' m0 '},\n",
      " {'char1ID': 'u0 ',\n",
      "  'char2ID': ' u2 ',\n",
      "  'dialoguesIDs': \" ['L204', 'L205', 'L206']\",\n",
      "  'lines': [{'charID': ' u2 ',\n",
      "             'char_name': ' CAMERON ',\n",
      "             'lineID': 'L204',\n",
      "             'movieID': ' m0 ',\n",
      "             'text': ' Why?'},\n",
      "            {'charID': ' u0 ',\n",
      "             'char_name': ' BIANCA ',\n",
      "             'lineID': 'L205',\n",
      "             'movieID': ' m0 ',\n",
      "             'text': ' Unsolved mystery.  She used to be really popular when '\n",
      "                     'she started high school, then it was just like she got '\n",
      "                     'sick of it or something.'},\n",
      "            {'charID': ' u2 ',\n",
      "             'char_name': ' CAMERON ',\n",
      "             'lineID': 'L206',\n",
      "             'movieID': ' m0 ',\n",
      "             'text': \" That's a shame.\"}],\n",
      "  'movieID': ' m0 '},\n",
      " {'char1ID': 'u0 ',\n",
      "  'char2ID': ' u2 ',\n",
      "  'dialoguesIDs': \" ['L207', 'L208']\",\n",
      "  'lines': [{'charID': ' u0 ',\n",
      "             'char_name': ' BIANCA ',\n",
      "             'lineID': 'L207',\n",
      "             'movieID': ' m0 ',\n",
      "             'text': ' Gosh, if only we could find Kat a boyfriend...'},\n",
      "            {'charID': ' u2 ',\n",
      "             'char_name': ' CAMERON ',\n",
      "             'lineID': 'L208',\n",
      "             'movieID': ' m0 ',\n",
      "             'text': ' Let me see what I can do.'}],\n",
      "  'movieID': ' m0 '},\n",
      " {'char1ID': 'u0 ',\n",
      "  'char2ID': ' u2 ',\n",
      "  'dialoguesIDs': \" ['L271', 'L272', 'L273', 'L274', 'L275']\",\n",
      "  'lines': [{'charID': ' u0 ',\n",
      "             'char_name': ' BIANCA ',\n",
      "             'lineID': 'L271',\n",
      "             'movieID': ' m0 ',\n",
      "             'text': \" C'esc ma tete. This is my head\"},\n",
      "            {'charID': ' u2 ',\n",
      "             'char_name': ' CAMERON ',\n",
      "             'lineID': 'L272',\n",
      "             'movieID': ' m0 ',\n",
      "             'text': \" Right.  See?  You're ready for the quiz.\"},\n",
      "            {'charID': ' u0 ',\n",
      "             'char_name': ' BIANCA ',\n",
      "             'lineID': 'L273',\n",
      "             'movieID': ' m0 ',\n",
      "             'text': \" I don't want to know how to say that though.  I want to \"\n",
      "                     'know useful things. Like where the good stores are.  How '\n",
      "                     'much does champagne cost?  Stuff like Chat.  I have '\n",
      "                     'never in my life had to point out my head to someone.'},\n",
      "            {'charID': ' u2 ',\n",
      "             'char_name': ' CAMERON ',\n",
      "             'lineID': 'L274',\n",
      "             'movieID': ' m0 ',\n",
      "             'text': \" That's because it's such a nice one.\"},\n",
      "            {'charID': ' u0 ',\n",
      "             'char_name': ' BIANCA ',\n",
      "             'lineID': 'L275',\n",
      "             'movieID': ' m0 ',\n",
      "             'text': ' Forget French.'}],\n",
      "  'movieID': ' m0 '},\n",
      " {'char1ID': 'u0 ',\n",
      "  'char2ID': ' u2 ',\n",
      "  'dialoguesIDs': \" ['L276', 'L277']\",\n",
      "  'lines': [{'charID': ' u0 ',\n",
      "             'char_name': ' BIANCA ',\n",
      "             'lineID': 'L276',\n",
      "             'movieID': ' m0 ',\n",
      "             'text': ' How is our little Find the Wench A Date plan '\n",
      "                     'progressing?'},\n",
      "            {'charID': ' u2 ',\n",
      "             'char_name': ' CAMERON ',\n",
      "             'lineID': 'L277',\n",
      "             'movieID': ' m0 ',\n",
      "             'text': \" Well, there's someone I think might be --\"}],\n",
      "  'movieID': ' m0 '},\n",
      " {'char1ID': 'u0 ',\n",
      "  'char2ID': ' u2 ',\n",
      "  'dialoguesIDs': \" ['L280', 'L281']\",\n",
      "  'lines': [{'charID': ' u2 ',\n",
      "             'char_name': ' CAMERON ',\n",
      "             'lineID': 'L280',\n",
      "             'movieID': ' m0 ',\n",
      "             'text': ' There.'},\n",
      "            {'charID': ' u0 ',\n",
      "             'char_name': ' BIANCA ',\n",
      "             'lineID': 'L281',\n",
      "             'movieID': ' m0 ',\n",
      "             'text': ' Where?'}],\n",
      "  'movieID': ' m0 '},\n",
      " {'char1ID': 'u0 ',\n",
      "  'char2ID': ' u2 ',\n",
      "  'dialoguesIDs': \" ['L363', 'L364']\",\n",
      "  'lines': [{'charID': ' u2 ',\n",
      "             'char_name': ' CAMERON ',\n",
      "             'lineID': 'L363',\n",
      "             'movieID': ' m0 ',\n",
      "             'text': ' You got something on your mind?'},\n",
      "            {'charID': ' u0 ',\n",
      "             'char_name': ' BIANCA ',\n",
      "             'lineID': 'L364',\n",
      "             'movieID': ' m0 ',\n",
      "             'text': ' I counted on you to help my cause. You and that thug '\n",
      "                     \"are obviously failing. Aren't we ever going on our \"\n",
      "                     'date?'}],\n",
      "  'movieID': ' m0 '},\n",
      " {'char1ID': 'u0 ',\n",
      "  'char2ID': ' u2 ',\n",
      "  'dialoguesIDs': \" ['L365', 'L366']\",\n",
      "  'lines': [{'charID': ' u2 ',\n",
      "             'char_name': ' CAMERON ',\n",
      "             'lineID': 'L365',\n",
      "             'movieID': ' m0 ',\n",
      "             'text': ' You have my word.  As a gentleman'},\n",
      "            {'charID': ' u0 ',\n",
      "             'char_name': ' BIANCA ',\n",
      "             'lineID': 'L366',\n",
      "             'movieID': ' m0 ',\n",
      "             'text': \" You're sweet.\"}],\n",
      "  'movieID': ' m0 '}]\n"
     ]
    }
   ],
   "source": [
    "conv_fields = ['char1ID', 'char2ID', 'movieID', 'dialoguesIDs']\n",
    "convs = []\n",
    "with open('movie_conversations.txt', 'r', encoding='iso-8859-1') as file:\n",
    "  for line in file:\n",
    "    vals = line.strip().split('+++$+++')\n",
    "    conv_obj = {}\n",
    "    for i in range(len(vals)):\n",
    "      conv_obj[conv_fields[i]] = vals[i]\n",
    "    lineIDs = conv_obj['dialoguesIDs'] # get line IDs from dialoguesIDs string, string type\n",
    "    lineIDs = lineIDs[2:-1] # remove braces from string \n",
    "    conv_obj[\"lines\"] = []\n",
    "    \n",
    "    for lineID in lineIDs.split(','): # split line IDs separated by ','\n",
    "      lineID = lineID.strip('\\' ')\n",
    "      try:\n",
    "        conv_obj[\"lines\"].append(lines[lineID]) #search for that line in lines dict with this linsID\n",
    "      except KeyError:\n",
    "        not_found += 1\n",
    "#         print(\"here\")\n",
    "        not_found.append(lineID)\n",
    "    convs.append(conv_obj)\n",
    "pp = pprint.PrettyPrinter()\n",
    "pp.pprint(convs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83097\n"
     ]
    }
   ],
   "source": [
    "print(len(convs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221282\n"
     ]
    }
   ],
   "source": [
    "#extract pairs of conversations\n",
    "conv_pairs = []\n",
    "for conv in convs :\n",
    "    for i in range(len(conv['lines'])-1):\n",
    "        conv1 = conv['lines'][i]['text']\n",
    "        conv2 = conv['lines'][i+1]['text']\n",
    "        if conv1 and conv2:\n",
    "            conv_pairs.append([conv1, conv2])\n",
    "print(len(conv_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new file containing conversation pairs\n",
    "delimiter = '\\t'\n",
    "delimiter = str(codecs.decode(delimiter, \"unicode_escape\"))\n",
    "with open(\"conv_pairs.txt\", \"w\", encoding='utf-8') as out_file:\n",
    "    writer = csv.writer(out_file, delimiter=delimiter)\n",
    "    for pair in conv_pairs:\n",
    "        writer.writerow(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221282\n",
      "b\" Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\\t Well, I thought we'd start with pronunciation, if that's okay with you.\\r\\n\"\n",
      "b\" Well, I thought we'd start with pronunciation, if that's okay with you.\\t Not the hacking and gagging and spitting part.  Please.\\r\\n\"\n",
      "b\" Not the hacking and gagging and spitting part.  Please.\\t Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\\r\\n\"\n",
      "b\" You're asking me out.  That's so cute. What's your name again?\\t Forget it.\\r\\n\"\n",
      "b\" No, no, it's my fault -- we didn't have a proper introduction ---\\t Cameron.\\r\\n\"\n",
      "b\" Cameron.\\t The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\\r\\n\"\n",
      "b\" The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\\t Seems like she could get a date easy enough...\\r\\n\"\n",
      "b' Why?\\t Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\\r\\n'\n",
      "b\" Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\\t That's a shame.\\r\\n\"\n",
      "b' Gosh, if only we could find Kat a boyfriend...\\t Let me see what I can do.\\r\\n'\n"
     ]
    }
   ],
   "source": [
    "with open('conv_pairs.txt','rb') as file:\n",
    "    lines = file.readlines()\n",
    "    print(len(lines))\n",
    "    for line in lines[:10]:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2 : **Processing the words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn unicode string to plain ascii\n",
    "\n",
    "# NFD : Normal Form Decomposed - decomposes the chars and combines it together\n",
    "# Mn : none marking space\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD',s) if unicodedata.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_string(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    \n",
    "    # replace any char(?!.) by whitespace + char\n",
    "    # \\1 means first bracketed group --> [?!.]\n",
    "    # r is not to consider \\1 as a character\n",
    "    # r to escape backslash\n",
    "    s = re.sub(r\"([?!.])\", r\" \\1\",s)\n",
    "    \n",
    "    # remove any char that's not a sequence of  [a-zA-z?!.] \n",
    "    s = re.sub(r\"[^a-zA-z?!.]+\", r\" \", s)\n",
    "    \n",
    "    # remove sequence of whitespace chars\n",
    "    s = re.sub(r\"\\s+\", r\" \", s)\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'frjf ? ! .'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalise_string(\"frjf45  ? !.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading and processing file\n",
      "done processing\n"
     ]
    }
   ],
   "source": [
    "datafile = 'conv_pairs.txt'\n",
    "\n",
    "# list 'lines' elements are conversation pairs\n",
    "lines = open(datafile, encoding='utf-8').read().strip().split('\\n')\n",
    "print(\"reading and processing file\")\n",
    "\n",
    "# for each dialogue in the pair, normalise the dialogue string\n",
    "# pairs of normalised strings are obtained\n",
    "pairs = [[normalise_string(s) for s in pairs.split('\\t')] for pairs in lines]\n",
    "\n",
    "print(\"done processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['can we make this quick ? roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad . again .',\n",
       "  'well i thought we d start with pronunciation if that s okay with you .'],\n",
       " ['well i thought we d start with pronunciation if that s okay with you .',\n",
       "  'not the hacking and gagging and spitting part . please .'],\n",
       " ['not the hacking and gagging and spitting part . please .',\n",
       "  'okay . . . then how bout we try out some french cuisine . saturday ? night ?'],\n",
       " ['you re asking me out . that s so cute . what s your name again ?',\n",
       "  'forget it .'],\n",
       " ['no no it s my fault we didn t have a proper introduction ', 'cameron .'],\n",
       " ['cameron .',\n",
       "  'the thing is cameron i m at the mercy of a particularly hideous breed of loser . my sister . i can t date until she does .'],\n",
       " ['the thing is cameron i m at the mercy of a particularly hideous breed of loser . my sister . i can t date until she does .',\n",
       "  'seems like she could get a date easy enough . . .'],\n",
       " ['why ?',\n",
       "  'unsolved mystery . she used to be really popular when she started high school then it was just like she got sick of it or something .'],\n",
       " ['unsolved mystery . she used to be really popular when she started high school then it was just like she got sick of it or something .',\n",
       "  'that s a shame .'],\n",
       " ['gosh if only we could find kat a boyfriend . . .',\n",
       "  'let me see what i can do .']]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221282"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep conversation pairs where dialogue length <= 10 words, check both dialogues\n",
    "\n",
    "MAX_LENGTH = 10\n",
    "def filter_pairs(s):\n",
    "    # check that both dialogues' length < 2= 10\n",
    "    return len(s[0].split()) <= MAX_LENGTH and len(s[1].split()) <= MAX_LENGTH\n",
    "\n",
    "# keep only those pairs that return true for above function\n",
    "pairs = [pair for pair in pairs if filter_pairs(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75024"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes 'pairs' - list of pair of dialogues,return max length of any dialogue in pairs\n",
    "def max_len_of_dialogue(pairs):\n",
    "    max_len = 0\n",
    "    for pair in pairs[:10]:\n",
    "        p1 = pair[0].split()\n",
    "        p2 = pair[1].split()\n",
    "        l1 = len(p1)\n",
    "        l2 = len(p2)\n",
    "        max_len = l1 if l1 > max_len else l2 if l2 > max_len else max_len\n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [pair for pair in pairs if len(pair)>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75024"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "\n",
    "class Vocabulary():\n",
    "    def __init__(self, name):\n",
    "        self.name = name # assign name to different vocabularies, if there are > 1\n",
    "        self.word2ind = {} # dictionary to store unique index for each word in dictionary{word:id}\n",
    "        self.word2count = {} # store frequency of each word\n",
    "        self.ind2word = {PAD_token:\"PAD\", SOS_token:\"SOS\", EOS_token:\"EOS\"}  #id:word\n",
    "        self.num_words = 3 #PAD, SOS, EOS\n",
    "    \n",
    "    def word_count(self, sentence): # function to count word frequency and add word in vocabulary\n",
    "        for word in sentence.split(' '):\n",
    "            try: # if the word exists, increment its count\n",
    "                self.word2count[word] += 1 \n",
    "            except KeyError: # if the word doesnot already exist in vocabulary\n",
    "                self.word2count[word] = 1\n",
    "                #self.add_word(word)\n",
    "    \n",
    "    def add_word(self, word):\n",
    "        self.word2ind[word] = self.num_words \n",
    "        self.ind2word[self.num_words] = word\n",
    "        self.num_words += 1\n",
    "            \n",
    "    def __str__(self):\n",
    "        return \"Vocabulary size: {}\".format(str(self.num_words))\n",
    "    \n",
    "    # remove ethose words from vocabulary whose frequency is below certain threshold\n",
    "    def trim(self, threshold):\n",
    "        deleted_words = 0\n",
    "        for k,v in self.word2count.items():\n",
    "            if v < threshold:\n",
    "                deleted_words += 1\n",
    "                self.word2count[k]= 0 # set frequency of word in word2count dict = 0\n",
    "            else:\n",
    "                self.add_word(k)\n",
    "        #for word in self.word2count.keys():\n",
    "         #   if self.word2count[word] > 0:\n",
    "          #      self.add_word(word)\n",
    "        \n",
    "        words_in_vocab = len(self.word2ind)\n",
    "        words_in_corpus = words_in_vocab + deleted_words\n",
    "    \n",
    "        print(\"keep words {}/{} = {:.4f}\".format(words_in_vocab, words_in_corpus, words_in_vocab/words_in_corpus ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary(\"cornell-movie-dialogues-corpus\")\n",
    "MIN_FREQ = 3 # only keep words with frequqncy >= 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep words 9045/20173 = 0.4484\n"
     ]
    }
   ],
   "source": [
    "# add words to vocab whose frequency >= 3\n",
    "for pair in pairs:\n",
    "    vocab.word_count(pair[0])\n",
    "    vocab.word_count(pair[1])\n",
    "    \n",
    "vocab.trim(MIN_FREQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75024"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pairs(): # remove those pairs whose any of the 2 dialogues contains infrequent words\n",
    "    keep_pairs = []\n",
    "    \n",
    "    for pair in pairs:\n",
    "        keep_first = True # flag to keep or remove 1st dialogue\n",
    "        keep_sec = True# flag to keep or remove 2nd dialogue\n",
    "        for word in pair[0].split(): # 1st dialogue\n",
    "            if word not in vocab.word2ind:\n",
    "                keep_first = False\n",
    "                break\n",
    "        for word in pair[1].split(): # 2nd dialogue\n",
    "            if word not in vocab.word2ind:\n",
    "                keep_sec = False\n",
    "                break\n",
    "        if keep_first and keep_sec :\n",
    "             keep_pairs.append(pair)\n",
    "    return keep_pairs\n",
    "    #print(\"pairs earlier : {}, pairs now: {}\".format(len(pairs),len(keep_pairs)))\n",
    "\n",
    "pairs = remove_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62751\n"
     ]
    }
   ],
   "source": [
    "print(len(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexes_from_sentences(voc_obj, sentence):\n",
    "    return [voc_obj.word2ind[word] for word in sentence.split()]+[EOS_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "burt reynolds .\n",
      "[7045, 3376, 11, 2]\n"
     ]
    }
   ],
   "source": [
    "print(pairs[60000][0])\n",
    "print(indexes_from_sentences(vocab, pairs[60000][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1277"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.word2ind['interested']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 9048\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['that s because it s such a nice one .', 'there .', 'you have my word . as a gentleman', 'hi .', 'have fun tonight ?', 'well no . . .', 'then that s all you had to say .', 'but', 'do you listen to this crap ?', 'what good stuff ?']\n",
      "[[3, 4, 5, 6, 4, 7, 8, 9, 10, 11, 2], [14, 11, 2], [17, 18, 19, 20, 11, 21, 8, 22, 2], [25, 11, 2], [18, 40, 31, 16, 2], [42, 43, 11, 11, 11, 2], [44, 3, 4, 45, 17, 46, 47, 48, 11, 2], [49, 2], [54, 17, 55, 47, 52, 56, 16, 2], [57, 58, 59, 16, 2]]\n"
     ]
    }
   ],
   "source": [
    "inp = []\n",
    "for pair in pairs[:10]:\n",
    "    inp.append(pair[0])\n",
    "print(inp)\n",
    "ind = [indexes_from_sentences(vocab, sentence) for sentence in inp]\n",
    "print(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EOS'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.ind2word[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<itertools.zip_longest at 0x7f1c60d6bd18>"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ['a', 'b', 'c', 'd', 'e']\n",
    "b = [1,2,3]\n",
    "c = itertools.zip_longest(a,b)\n",
    "c\n",
    "#list(c) # an iterator is returned which is then converted to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a', 1)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(c) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 2), ('c', 3), ('d', None), ('e', None)]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 14, 17, 25, 18, 42, 44, 49, 54, 57),\n",
       " (4, 11, 18, 11, 40, 43, 3, 2, 17, 58),\n",
       " (5, 2, 19, 2, 31, 11, 4, 0, 55, 59),\n",
       " (6, 0, 20, 0, 16, 11, 45, 0, 47, 16),\n",
       " (4, 0, 11, 0, 2, 11, 17, 0, 52, 2),\n",
       " (7, 0, 21, 0, 0, 2, 46, 0, 56, 0),\n",
       " (8, 0, 8, 0, 0, 0, 47, 0, 16, 0),\n",
       " (9, 0, 22, 0, 0, 0, 48, 0, 2, 0),\n",
       " (10, 0, 2, 0, 0, 0, 11, 0, 0, 0),\n",
       " (11, 0, 0, 0, 0, 0, 2, 0, 0, 0),\n",
       " (2, 0, 0, 0, 0, 0, 0, 0, 0, 0)]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(itertools.zip_longest(*ind, fillvalue=0)) # zip values colummn-wise, fill 0 at places where None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_padding(l, fillvalue=0): # default padding is with 0s\n",
    "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 4, 5, 6, 4, 7, 8, 9, 10, 11, 2], [14, 11, 2], [17, 18, 19, 20, 11, 21, 8, 22, 2], [25, 11, 2], [18, 40, 31, 16, 2], [42, 43, 11, 11, 11, 2], [44, 3, 4, 45, 17, 46, 47, 48, 11, 2], [49, 2], [54, 17, 55, 47, 52, 56, 16, 2], [57, 58, 59, 16, 2]]\n"
     ]
    }
   ],
   "source": [
    "print(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max length will be 11 as EOS token is also appended at the end of each sentence\n",
    "leng = [len(index) for index in ind]\n",
    "max(leng) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(3, 14, 17, 25, 18, 42, 44, 49, 54, 57),\n",
       " (4, 11, 18, 11, 40, 43, 3, 2, 17, 58),\n",
       " (5, 2, 19, 2, 31, 11, 4, 0, 55, 59),\n",
       " (6, 0, 20, 0, 16, 11, 45, 0, 47, 16),\n",
       " (4, 0, 11, 0, 2, 11, 17, 0, 52, 2),\n",
       " (7, 0, 21, 0, 0, 2, 46, 0, 56, 0),\n",
       " (8, 0, 8, 0, 0, 0, 47, 0, 16, 0),\n",
       " (9, 0, 22, 0, 0, 0, 48, 0, 2, 0),\n",
       " (10, 0, 2, 0, 0, 0, 11, 0, 0, 0),\n",
       " (11, 0, 0, 0, 0, 0, 2, 0, 0, 0),\n",
       " (2, 0, 0, 0, 0, 0, 0, 0, 0, 0)]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded = zero_padding(ind) # in padded, max_len(11) is now the number of rows, #cols = batch size\n",
    "print(len(padded))\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find actual words in list l, if padded_token found(value 0),put 0, else 1\n",
    "def binary_padding(l):\n",
    "    m = []\n",
    "    for i,seq in enumerate(padded):\n",
    "        m.append([])\n",
    "        for ind in seq:\n",
    "            if ind == PAD_token:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [1, 1, 1, 1, 1, 1, 1, 0, 1, 1],\n",
       " [1, 0, 1, 0, 1, 1, 1, 0, 1, 1],\n",
       " [1, 0, 1, 0, 1, 1, 1, 0, 1, 1],\n",
       " [1, 0, 1, 0, 0, 1, 1, 0, 1, 0],\n",
       " [1, 0, 1, 0, 0, 0, 1, 0, 1, 0],\n",
       " [1, 0, 1, 0, 0, 0, 1, 0, 1, 0],\n",
       " [1, 0, 1, 0, 0, 0, 1, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_padding(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_var(l, vocab):\n",
    "    indexes_batch = [indexes_from_sentences(vocab, sentence) for sentence in l]\n",
    "    lengths = torch.tensor([len(ind) for ind in indexes_batch])\n",
    "    pad_list = zero_padding(indexes_batch)\n",
    "    pad_var = torch.LongTensor(pad_list)\n",
    "    return pad_var, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_var(l, vocab):\n",
    "    indexes_batch = [indexes_from_sentences(vocab, sentence) for sentence in l]\n",
    "    max_target_len = max([len(ind) for ind in indexes_batch])\n",
    "    pad_list = zero_padding(indexes_batch)\n",
    "    mask = torch.ByteTensor(binary_padding(pad_list))\n",
    "    pad_var = torch.LongTensor(pad_list)\n",
    "    return max_target_len, mask, pad_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch2train_data(batch_pairs, vocab):\n",
    "    \n",
    "    # arrange pairs in decreasing order of length of 1st dialogue(question)\n",
    "    batch_pairs.sort(key = lambda x : len(x[0].split()), reverse=True)\n",
    "    ques_batch,replies_batch = [],[]\n",
    "    for pair in batch_pairs:\n",
    "        ques_batch.append(pair[0])\n",
    "        replies_batch.append(pair[1])\n",
    "    inp, lengths = input_var(ques_batch, vocab)\n",
    "    max_target_len, mask, output = output_var(replies_batch, vocab)\n",
    "    return inp, lengths, max_target_len, mask, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 : Defining the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder in GRU only gives one output - the hidden state output. \n",
    "\n",
    "    For encoder:\n",
    "    Steps to be taken are : convert word indexes to embeddings, Pack padded batch of sequences for RNN module, Forward pass through GRU, Unpack padding, Finally return output and final hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        #the input is word embedding whose size is that of hidden layer\n",
    "        #arguments - input size, hidden size, n_layers, dropout, bidirectional(default value-False)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers==1 else dropout))\n",
    "        \n",
    "    def forward(self, inp_seq, inp_length, hidden=None):\n",
    "        # inp_seq shape(which will be pack padded seq) = (batch_size,max_len)\n",
    "        # inp_lengths : length of all input sentences in batch\n",
    "        # hidden(n_layers * num_directions, batch, hidden_size) : initial hidden state for each element in batch\n",
    "        \n",
    "        #convert word indexes to embeddings\n",
    "        embedded = self.embedding(inp_seq)\n",
    "        \n",
    "        #pack padded batch of sequences for RNN module\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, inp_length)\n",
    "        \n",
    "        #forward pass through GRU\n",
    "        #output shape : (seq len, batch_size, #directions * hidden_size)\n",
    "        # hidden shape : (#layers * #directions, batch_size, hidden_size) - hidden state from last time step -\n",
    "        # which basically encodes all the information of the Encoder, given as input to Decoder\n",
    "        output, hidden = self.gru(packed, hidden)\n",
    "        \n",
    "        #unpack padding\n",
    "        output, _ = torch.nn.utils.rnn.pad_packed_sequence(output)\n",
    "        \n",
    "        #return final output and final hidden state\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-6.9805e-01, -1.7681e+00,  1.0990e+00, -1.4743e+00,  3.0581e-02,\n",
      "          -7.8625e-01],\n",
      "         [-1.5699e+00,  1.6225e-01, -1.7348e+00, -5.1328e-01, -9.3138e-01,\n",
      "           9.9514e-01],\n",
      "         [-2.1086e+00,  9.9282e-01,  3.6357e-02,  1.6315e-01, -7.8567e-02,\n",
      "          -2.4133e+00]],\n",
      "\n",
      "        [[-1.9057e-01,  3.1464e-01, -1.7947e+00,  8.7771e-02, -8.9390e-01,\n",
      "           2.3887e-03],\n",
      "         [ 5.9791e-01, -1.3839e+00, -1.2177e-01,  1.3019e+00,  1.8720e+00,\n",
      "           2.0717e+00],\n",
      "         [-2.1651e-01,  6.8395e-01,  2.4968e+00, -3.5749e-01, -1.5751e+00,\n",
      "          -7.2547e-01]],\n",
      "\n",
      "        [[-4.1416e-01,  2.2015e+00,  7.4063e-01,  5.1939e-02, -8.8393e-01,\n",
      "          -1.2079e+00],\n",
      "         [-5.6117e-01, -1.5215e+00,  1.6828e+00,  2.6706e-02, -1.3378e+00,\n",
      "           2.0565e+00],\n",
      "         [-7.1246e-01,  1.1908e-01, -7.0539e-02,  4.9878e-01, -7.7186e-01,\n",
      "          -1.3203e+00]],\n",
      "\n",
      "        [[-8.9286e-01, -4.5664e-01, -1.3498e+00, -1.0421e+00, -9.1390e-01,\n",
      "          -5.2860e-01],\n",
      "         [-5.1223e-01,  1.6472e+00, -9.1353e-01, -2.3628e-01,  9.6580e-01,\n",
      "           6.5402e-01],\n",
      "         [-1.2809e+00, -1.1300e-01,  1.3577e-01,  4.4818e-01,  2.2225e+00,\n",
      "           1.2420e-01]],\n",
      "\n",
      "        [[ 7.1948e-02,  1.6736e+00, -1.1123e+00, -1.2332e-01,  7.1167e-01,\n",
      "          -1.3902e+00],\n",
      "         [ 1.0962e+00, -1.7078e+00,  5.3223e-01, -1.8374e+00,  1.0289e+00,\n",
      "           6.3031e-01],\n",
      "         [ 5.3215e-01, -8.3817e-01, -1.2746e+00,  3.3383e-01, -1.1033e+00,\n",
      "          -2.6595e+00]]])\n",
      "tensor([[[ 0.4695, -1.6151, -0.1030,  0.6845, -1.3848, -1.3594,  0.5431,\n",
      "           1.1505,  0.1240, -0.3370, -0.8764, -1.9340, -0.9104,  0.6646,\n",
      "           1.1185, -0.8086, -0.6382, -0.4343,  0.4340,  0.0546],\n",
      "         [ 0.8553, -1.3752,  0.9116,  1.3655,  1.4783, -0.3183, -0.0850,\n",
      "          -2.1189,  0.2587, -1.6695,  0.6322,  0.3718, -0.7030,  0.0947,\n",
      "          -0.6236,  1.2356,  0.4104,  1.1089, -0.1319, -0.0491],\n",
      "         [ 0.8234,  0.5372, -0.1570,  2.1117, -0.3818,  0.0131,  0.2533,\n",
      "          -0.8502,  0.1744, -0.0215,  0.8837,  1.8195, -0.3819,  0.4193,\n",
      "          -0.0434,  2.4875, -1.0064, -0.1187, -2.5098, -0.4247]],\n",
      "\n",
      "        [[ 1.0582, -0.8544, -0.3190, -0.4251,  0.2598,  1.9237,  0.6288,\n",
      "          -1.0040,  0.1004, -0.2954,  0.7806,  0.8954,  0.0961,  1.4068,\n",
      "          -0.3550, -1.5174, -1.3834, -3.0449,  1.4073, -0.9643],\n",
      "         [-0.1181,  1.7349,  0.3660,  1.9602,  0.5518, -1.1036,  0.0740,\n",
      "          -0.7390,  1.0363, -0.2495,  0.0392, -0.6902,  0.4828, -1.2812,\n",
      "           0.5524,  0.9030,  1.5526, -0.7583,  1.1825, -0.2174],\n",
      "         [ 0.6761,  0.3771,  1.1931, -0.5271, -2.0531, -0.5056,  0.2615,\n",
      "           1.1432, -0.5498,  0.4604,  1.0423,  0.1418,  0.8561,  0.8030,\n",
      "          -0.1993,  1.4027, -0.1687,  0.0914, -1.5425,  0.2085]]])\n",
      "torch.Size([5, 3, 20])\n",
      "torch.Size([2, 3, 20])\n"
     ]
    }
   ],
   "source": [
    "#input shape(seq_len, batch_size, input_dim(eg dim of embedded vector))\n",
    "inp = torch.randn(5, 3, 6)\n",
    "\n",
    "#initial hidden state shape (#layers, batch_size, hidden_size)\n",
    "h0 = torch.randn(2, 3, 20) \n",
    "\n",
    "model = nn.GRU(6, 20, 2) # (input_size, hidden_size, #layers)\n",
    "\n",
    "out, hn = model(inp, h0)\n",
    "print(inp)\n",
    "print(h0)\n",
    "print(out.shape) #(seq_len, batch_size, hidden_size)\n",
    "print(hn.shape) #(#layers, batch_size, hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pack padded sequence in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(6, 7) # say batch_size=6 and max_len =7 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([0.8499, 0.6362, 0.0764, 0.1271, 0.0209, 0.5739, 0.1687, 0.3382, 0.0887,\n",
       "        0.6734, 0.8333, 0.8849, 0.5991, 0.8688, 0.9042, 0.3571, 0.2051, 0.3043,\n",
       "        0.6446, 0.5782, 0.6829, 0.7207, 0.1120, 0.8136, 0.9847, 0.8700, 0.2159,\n",
       "        0.2947, 0.3015, 0.7877, 0.4076]), batch_sizes=tensor([6, 6, 5, 5, 4, 3, 2]), sorted_indices=None, unsorted_indices=None)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths = [7,7,6,5,4,2] # sentence length for 6 sentences in our batch\n",
    "\n",
    "# batch_first indicates that input has 1st dim as batch_size\n",
    "nn.utils.rnn.pack_padded_sequence(a, lengths, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8499, 0.1687, 0.5991, 0.3043, 0.1120, 0.2159, 0.7877],\n",
       "        [0.6362, 0.3382, 0.8688, 0.6446, 0.8136, 0.2947, 0.4076],\n",
       "        [0.0764, 0.0887, 0.9042, 0.5782, 0.9847, 0.3015, 0.5674],\n",
       "        [0.1271, 0.6734, 0.3571, 0.6829, 0.8700, 0.2307, 0.1862],\n",
       "        [0.0209, 0.8333, 0.2051, 0.7207, 0.2242, 0.6115, 0.3358],\n",
       "        [0.5739, 0.8849, 0.9354, 0.2457, 0.4412, 0.1774, 0.3433]])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder with attention mechanism\n",
    "\n",
    "The decoder RNN generates the response sentence in a token-by-token fashion. It uses the encoder’s context vectors, and internal hidden states to generate the next word in the sequence. It continues generating words until it outputs an EOS_token, representing the end of the sentence. A common problem with a vanilla seq2seq decoder is that if we rely soley on the context vector to encode the entire input sequence’s meaning, it is likely that we will have information loss. This is especially the case when dealing with long input sequences, greatly limiting the capability of our decoder.\n",
    "\n",
    "To combat this, Bahdanau et al. created an “attention mechanism” that allows the decoder to pay attention to certain parts of the input sequence, rather than using the entire fixed context at every step.\n",
    "\n",
    "At a high level, attention is calculated using the decoder’s current hidden state and the encoder’s outputs. The output attention weights have the same shape as the input sequence, allowing us to multiply them by the encoder outputs, giving us a weighted sum which indicates the parts of encoder output to pay attention to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "    \n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        return torch.sum(hidden * encoder_output, dim=2) # sum across dim 2\n",
    "    \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \n",
    "        # hidden shape : (1, batch_size, hidden_size)\n",
    "        # encoder_outputs shape : (max_len, batch_size, hidden_size)\n",
    "        # element-wise multiplication gives tensor of shape (max_len, batch_size, hidden_size)\n",
    "        attn_energies = self.dot_score(hidden, encoder_outputs) # (max_len, batch_size)\n",
    "        \n",
    "        #transpose\n",
    "        attn_energies = attn_energies.t()\n",
    "        \n",
    "        # return softmax normalised proabilities \n",
    "        # each row will add up to 1 (dim 1)\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1) #(batch_size, 1, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2360, 0.3711, 0.3929],\n",
      "        [0.1039, 0.1753, 0.7208],\n",
      "        [0.2209, 0.2284, 0.5508],\n",
      "        [0.2085, 0.4247, 0.3668],\n",
      "        [0.1816, 0.3150, 0.5034]])\n",
      "tensor([[[0.2360, 0.3711, 0.3929]],\n",
      "\n",
      "        [[0.1039, 0.1753, 0.7208]],\n",
      "\n",
      "        [[0.2209, 0.2284, 0.5508]],\n",
      "\n",
      "        [[0.2085, 0.4247, 0.3668]],\n",
      "\n",
      "        [[0.1816, 0.3150, 0.5034]]])\n",
      "torch.Size([5, 1, 3])\n",
      "torch.Size([1, 3])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(5,3)\n",
    "b = F.softmax(a, dim=1)\n",
    "print(b)\n",
    "c = b.unsqueeze(1)\n",
    "print(c)\n",
    "print(c.shape)\n",
    "print(c[0].shape)\n",
    "print(b[0].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.attn_model = attn_model\n",
    "        self.embedding = embedding\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        #output_size = vocab size(softmax probabilities are predicted for each word in vocab)\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        #Define layers\n",
    "        self.embedding = embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers==1 else dropout))\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.attn = Attn(attn_model, hidden_size)\n",
    "        \n",
    "        \"\"\"def forward(self, input_step, last_hidden_state, encoder_outputs):\n",
    "            # input_step : one time step of input sequence batch; shape = (1, batch_size)\n",
    "            # last_hidden_state : final hidden state output of encoder; shape : (#layers * #directions, batch_size, hidden_size)\n",
    "            # encoder_outputs shape : (seq len, batch_size, #directions * hidden_size)\n",
    "            # this is run one step at a time (1 batch of words at a time)\n",
    "            embedded = self.embedding(input_step)\n",
    "            embedded = self.embedding_dropout(embedded)\n",
    "            \n",
    "            #rnn_output shape : (1,  batch_size, hidden_size)\n",
    "            #hidden output shape : (#layers, batch_size, hidden_size)\n",
    "            rnn_output, hidden = self.gru(embedded, last_hidden_state)\n",
    "            \n",
    "            \n",
    "            #calculate attention weights from current timestep\n",
    "            attn_wts = self.attn(rnn_output, encoder_outputs) # calls forward method of Attn class\n",
    "            \n",
    "            #multiply attention weights to encoder outputs to get weighted context vector\n",
    "            #(batch_size, 1, max_len) bmm with (batch_size, max_len, hidden) = (batch_size, 1, hidden)\n",
    "            context = attn_wts.bmm(encoder_outputs.transpose(0, 1))\n",
    "            \n",
    "            # squeeze both. now shape of both = (batch_size, hidden_size)\n",
    "            rnn_output = rnn_output.squeeze(0)\n",
    "            context = context.squeeze(1)\n",
    "            \n",
    "            #concatenate the 2 vectors along dimension 1; shape (batch_size, hidden_size*2)\n",
    "            concat_input = torch.cat((rnn_output, context), 1)\n",
    "            concat_output = torch.tanh(self.concat(concat_input))\n",
    "            \n",
    "            # output shape : (batch_size, vocab_size); next word predicted for every word in the batch \n",
    "            output = self.out(concat_output) \n",
    "            output = F.softmax(output, dim=1)\n",
    "            \n",
    "            return output, hidden\"\"\"\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the loss function\n",
    "\n",
    "Since we are dealing with batches of padded sequences, we can't consider all the elements in the tensor to calculate the loss. Loss will be calculated based on decoder output sequence, target(expected output) tensor and binary mask tensor describing the padding. Loss function calculates average of the negative log likelihood of elements that have corresponding '1' in mask tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(output, target, mask):\n",
    "    total = mask.sum() # total number of 1s (i.e total elements)\n",
    "    target = target.view(-1, 1) # shape becomes (batch_size, 1)\n",
    "    \n",
    "    gathered_tensor = torch.gather(output, 1, target)\n",
    "    \n",
    "    cross_entropy = -torch.log(gathered_tensor)\n",
    "    loss = cross_entropy.masked_select(mask)\n",
    "    loss = loss.mean().to(device)\n",
    "    \n",
    "    return loss, total.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62751"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['can i see you again ?', 'certainly not .'], ['just uh just slip the rent under my door .', 'yeah okay .'], ['when ?', 'i don t know . soon .'], ['you mean because his career s ruined and all ?', 'jesus . is that what he told you ?'], ['we ll speak soon .', ' night .']]\n",
      "inp tensor([[  86,   17,  114,   36,  249],\n",
      "        [ 267,  289,   34,  161,   16],\n",
      "        [  86,    5,  394,  451,    2],\n",
      "        [2618,  282,   17, 1238,    0],\n",
      "        [  61,  350,  708,   11,    0],\n",
      "        [5531,    4,   16,    2,    0],\n",
      "        [ 701, 4400,    2,    0,    0],\n",
      "        [  19,  148,    0,    0,    0],\n",
      "        [1551,   45,    0,    0,    0],\n",
      "        [  11,   16,    0,    0,    0],\n",
      "        [   2,    2,    0,    0,    0]])\n",
      "lengths tensor([11, 11,  7,  6,  3])\n",
      "max_target_len 10\n",
      "mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 1, 1],\n",
      "        [1, 0, 1, 0, 1, 1, 1, 0, 1, 1],\n",
      "        [1, 0, 1, 0, 1, 1, 1, 0, 1, 1],\n",
      "        [1, 0, 1, 0, 0, 1, 1, 0, 1, 0],\n",
      "        [1, 0, 1, 0, 0, 0, 1, 0, 1, 0],\n",
      "        [1, 0, 1, 0, 0, 0, 1, 0, 1, 0],\n",
      "        [1, 0, 1, 0, 0, 0, 1, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.uint8)\n",
      "output tensor([[ 197,  627, 2747,  137,   34],\n",
      "        [  70,   11,   75,   11,  102],\n",
      "        [  11,  142,   11,    2,  103],\n",
      "        [   2,    3,    2,    0,   33],\n",
      "        [   0,   57,    0,    0,   11],\n",
      "        [   0,   84,    0,    0, 1238],\n",
      "        [   0,  117,    0,    0,   11],\n",
      "        [   0,   17,    0,    0,    2],\n",
      "        [   0,   16,    0,    0,    0],\n",
      "        [   0,    2,    0,    0,    0]])\n",
      "Encoder output shape :  torch.Size([11, 5, 500])\n",
      "Encoder hidden shape :  torch.Size([2, 5, 500])\n",
      "decoder input shape :  torch.Size([1, 5])\n",
      "decoder input :  tensor([[1, 1, 1, 1, 1]])\n",
      "initial decoder hidden state shape :  torch.Size([2, 5, 500])\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-195-7426feebd963>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m#teacher forcing : in decoder, expected target word is given as input at each timestep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_target_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mdecoder_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Decoder output shape : \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoder_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Decoder hidden state shape : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *input)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mregistered\u001b[0m \u001b[0mhooks\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlatter\u001b[0m \u001b[0msilently\u001b[0m \u001b[0mignores\u001b[0m \u001b[0mthem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \"\"\"\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bs = 5\n",
    "\n",
    "# randomly select 5 dialogue pairs from 'pairs'\n",
    "test_pairs = [random.choice(pairs) for _ in range(bs)]\n",
    "print(test_pairs)\n",
    "inp, lengths, max_target_len, mask, output = batch2train_data(test_pairs,vocab) \n",
    "print(\"inp\", inp)\n",
    "print(\"lengths\", lengths)\n",
    "print(\"max_target_len\", max_target_len)\n",
    "print(\"mask\", mask)\n",
    "print(\"output\", output)\n",
    "\n",
    "# declare hyperparameters\n",
    "hidden_size = 500\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 2\n",
    "attn_model = 'dot'\n",
    "lr = 0.0001\n",
    "embedding = nn.Embedding(vocab.num_words, hidden_size)\n",
    "dropout = 0.1\n",
    " \n",
    "#encoder and decoder layers\n",
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "decoder = DecoderRNN(attn_model, embedding, hidden_size, vocab.num_words, decoder_n_layers, dropout)\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "#initialise optimisers\n",
    "encoder_optimiser = optim.Adam(encoder.parameters(), lr=lr)\n",
    "decoder_optimiser = optim.Adam(decoder.parameters(), lr=lr)\n",
    "encoder_optimiser.zero_grad()\n",
    "decoder_optimiser.zero_grad()\n",
    "\n",
    "# inp = inp.to(device)\n",
    "# lengths = lengths.to(device)\n",
    "# output = output.to(device)\n",
    "# mask = mask.to(device)\n",
    "\n",
    "loss = 0\n",
    "print_losses = []\n",
    "n_totals = 0\n",
    "\n",
    "encoder_out, encoder_hid = encoder(inp, lengths)\n",
    "print(\"Encoder output shape : \", encoder_out.shape)\n",
    "print(\"Encoder hidden shape : \", encoder_hid.shape)\n",
    "\n",
    "decoder_inp = torch.LongTensor([[SOS_token for _ in range(bs)]]).to(device)\n",
    "print(\"decoder input shape : \", decoder_inp.shape)\n",
    "print(\"decoder input : \",decoder_inp)\n",
    "decoder_hid = encoder_hid[:decoder_n_layers]\n",
    "print(\"initial decoder hidden state shape : \", decoder_hid.shape)\n",
    "print(\"\\n\")\n",
    "\n",
    "#teacher forcing : in decoder, expected target word is given as input at each timestep\n",
    "for t in range(max_target_len):\n",
    "    decoder_out, decoder_hid = decoder(decoder_inp, decoder_hid, encoder_out)\n",
    "    print(\"Decoder output shape : \",decoder_out.shape)\n",
    "    print(\"Decoder hidden state shape : \", decoder_hid.shape)\n",
    "    \n",
    "    print(\"target variable at current timestep : \", target_var[t])\n",
    "    print(\"shape of target variable at current timestep : \", target_var[t].shape)\n",
    "    decoder_inp = target_var[t].view(1, -1)\n",
    "    print(\"decoder input shape : \", decoder_inp.shape)\n",
    "    \n",
    "    print(\"mask at current timestep : \", mask[t])\n",
    "    print(\"mask shape at current timestep : \", mask[t].shape)\n",
    "    timestep_loss, n_total = loss(decoder_out, target_var[t], mask[t])\n",
    "    print(\"loss at current time step : \",loss)\n",
    "    print(\"n_total : \", n_total)\n",
    "    \n",
    "    loss += timestep_loss\n",
    "    print_losses.append(timestep_loss.item() * n_total)\n",
    "    print(\"print losses list : \", print_losses)\n",
    "    n_totals += n_total\n",
    "    print(\"n_totals : \", n_totals)\n",
    "    returned_loss = sum(print_losses)/n_totals\n",
    "    print(\"returned loss : \" , returned_loss)\n",
    "    print(\"\\n-------DONE ONE TIME STEP--------------\\n\")\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    #update weights of encoder's and decoder's weight matrices\n",
    "    encoder_optimiser.step()\n",
    "    decoder_optimiser.step()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Building chatbot in PyTorch.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
